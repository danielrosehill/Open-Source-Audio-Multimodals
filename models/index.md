# Audio Multimodal Models Index

Open-source models that process audio tokens natively alongside text prompts, enabling unified inference for transcription, analysis, and understanding.

| Model | Developer | Parameters | License | Notes |
|-------|-----------|------------|---------|-------|
| [Kimi-Audio](kimi-audio.md) | Moonshot AI | - | - | Native audio multimodal with long-context support |
| [Phi-4-Multimodal](phi-4-multimodal.md) | Microsoft | ~14B | MIT | Efficient multimodal (audio + vision + text) |
| [Qwen2-Audio](qwen2-audio.md) | Alibaba | 7B | - | GGUF available, strong multilingual, practical for local deployment |
| [Soundwave](soundwave.md) | FreedomIntelligence | 9B | Apache 2.0 | Data-efficient (10k hours), outperforms Qwen2-Audio on benchmarks |
| [Step-Audio-R1](step-audio-r1.md) | StepFun | 33B | Apache 2.0 | First audio LLM with Chain-of-Thought reasoning |
| [Ultravox](ultravox.md) | Fixie.ai | 1B-27B | Open | Multiple backbones (Llama, Gemma, Qwen), ~150ms TTFT |
| [Voxtral](voxtral.md) | Mistral AI | 3B-24B | Apache 2.0 | 30-min audio support, built-in function calling |
